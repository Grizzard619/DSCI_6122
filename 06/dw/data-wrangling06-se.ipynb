{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "084d12c7-68aa-4287-97e6-21aa1d7aede5",
   "metadata": {},
   "source": [
    "# Applied Data Wrangling 06\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this notebook, we'll take a look at the [IMDB Dataset of 50K Movie Reviews](https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews). We'll start by breifly exploring the data, then we'll clean it and perform some sentiment analysis. Next, we'll produce some word clouds using the `wordcloud` package. Finally, we'll train a classification model using tools from  `tidymodels` and `textrecipes` to classify positive and negative reviews.\n",
    "\n",
    "> The paper for the IMDB dataset can be found [here](https://ai.stanford.edu/~amaas/papers/wvSent_acl2011.pdf). The researchers only consider highly polarizing reviews in constructing their dataset: $\\leq$ 4 out of 10 is labeled negative while $\\geq$ out of 10 is labeled as positive. The dataset is balanced, so a random guess to whether a review was positive or negative has a 50% chance of being right. Additionally, only 30 reviews per movie were included in the dataset. The task for this dataset is to correctly predict the label (negative or positive) from the text of the review. In their 2011 paper, the best model the researchers tested had a classification accuracy of 88.89%.\n",
    "\n",
    "Before beginning this analysis, you'll need to...\n",
    "\n",
    "- download the `imdb-dataset.rds` from Canvas\n",
    "- install the `r-wordcloud=2.6` library\n",
    "- install the `r-textrecipes=1.0.7` library\n",
    "\n",
    "Be sure to specify the version number of the packages when you install. You should know the drill by now on how to install the packages. However, you'll find instructions on Canvas should you need them. Now, let's begin!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19523a24-7250-4e7f-a115-ae4868a5d40a",
   "metadata": {},
   "source": [
    "## Data Exploration and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527f1e86-18d1-4e5d-b346-6b122f77cb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "suppressMessages({\n",
    "    library(tidyverse)\n",
    "    library(tidytext)\n",
    "})\n",
    "\n",
    "tb_imdb = readRDS(\"imdb-dataset.rds\")\n",
    "\n",
    "tb_imdb %>% \n",
    "    str()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcbc737-f74e-49ea-b323-931ed7f01a84",
   "metadata": {},
   "source": [
    "We see that the dataset includes two columns: `review` and `sentiment`. The `review` column contains the text of the movie review, and the `sentiment` column indicates whether the review is positive or negative. Let's clean up the data a bit by tokenizing the reviews, removing stop words, and converting the `sentiment` column to a `factor` (binary instead of character):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c6da7b-bab4-4442-8109-f7a5e84f0d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanText = function(text) { # get rid of html newlines...\n",
    "    text %>%\n",
    "        str_replace_all(\"<br /><br />\", \"\") \n",
    "}\n",
    "\n",
    "tb_imdb_tokens = tb_imdb %>%\n",
    "    mutate(review = cleanText(review)) %>% \n",
    "    rowid_to_column(var = \"review_id\") %>% # label the reviews\n",
    "    mutate(sentiment = factor(sentiment, levels = c(\"negative\", \"positive\"))) %>% \n",
    "    rename(label = sentiment) %>% # change the name to avoid conflicts later\n",
    "    unnest_tokens(output = word, input = review) %>% \n",
    "    anti_join(stop_words, by = \"word\") # remove stop words\n",
    "\n",
    "tb_imdb_tokens %>% \n",
    "    str()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ab408d-6e07-4297-aa4a-847699394b27",
   "metadata": {},
   "source": [
    "As a first look at the data, is there any relationship between the number of words in a review and the sentiment of the review?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bfd778-514e-4266-972e-ac1c6dbdb863",
   "metadata": {},
   "outputs": [],
   "source": [
    "options(repr.plot.width = 12, repr.plot.height = 8)\n",
    "theme_set(theme_bw(base_size = 16))\n",
    "\n",
    "tb_imdb_tokens %>% \n",
    "    group_by(review_id) %>% \n",
    "    summarize(n_words = n(), label = first(label)) %>% \n",
    "    ggplot(aes(n_words, fill = label)) + \n",
    "    geom_density(color = \"NA\", alpha = 0.3) + \n",
    "    labs(title = \"distribution of # of words in a review\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcfa84e-9297-4f18-92e9-9410f8403fbc",
   "metadata": {},
   "source": [
    "Doesn't look like it; both positive and negative reviews have a similar distribution of word length. Next, let's look at the sentiment of reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59377f6b-31ac-4464-82c5-1bed6599c8e6",
   "metadata": {},
   "source": [
    "## Sentiment Analysis\n",
    "\n",
    "Now that we have cleaned data, let's perform some sentiment analysis.\n",
    "\n",
    "> What is sentiment analysis? Sentiment analysis is a field of Natural Language Processing (NLP) and text analysis that uses computational techniques to determine the emotional tone behind textual data. In our analysis, we'll be using a lexicon (\"bing\") that contains a list of words that have been labeled as positive or negative to calculate a sentiment score for each review. You can read more about it [here](https://en.wikipedia.org/wiki/Sentiment_analysis). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a932994-a071-425d-b2fc-d9ed539c9c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use inner_join to drop all \"neutral\" words\n",
    "# \"many-to-many\" used since some words can have positive and negative connotations\n",
    "tb_imdb_sentiment = tb_imdb_tokens %>%\n",
    "    inner_join(get_sentiments(\"bing\"), by = \"word\", relationship = \"many-to-many\") %>% \n",
    "    mutate(sentiment = ifelse(sentiment == \"negative\", 0, 1))\n",
    "\n",
    "tb_imdb_sentiment %>% \n",
    "    str()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb6609e-a5da-453e-9cfb-704323c95411",
   "metadata": {},
   "source": [
    "How many unique words are there, and what is there frequency?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075f8c69-d258-405a-8ac9-a36dc4d23d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_imdb_sentiment %>% \n",
    "    count(word, sort = T) %>% \n",
    "    mutate(p = n/sum(n)) %>% \n",
    "    head()\n",
    "\n",
    "tb_imdb_sentiment %>% \n",
    "    count(word, sort = T) %>% \n",
    "    rowid_to_column(var = \"word_index\") %>% \n",
    "    ggplot(aes(word_index, n)) + \n",
    "    geom_line() + \n",
    "    scale_y_continuous(trans = \"log10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b261aa9-3119-43d7-9a24-977fddcbb12e",
   "metadata": {},
   "source": [
    "Now let's look at the sentiment distribution score given the label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769b6ef3-d92e-4700-a9db-d82797c3df7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_imdb_sentiment %>% \n",
    "    group_by(review_id) %>% \n",
    "    summarize(mu_sentiment = mean(sentiment), label = first(label)) %>% \n",
    "    ggplot(aes(mu_sentiment, fill = label)) + \n",
    "    geom_density(color = \"NA\", alpha = 0.3) + \n",
    "    labs(title = \"distribution of review sentiment score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252afcd7-a936-4f56-abec-74761dcf134d",
   "metadata": {},
   "source": [
    "There's clearly a separation between the two distributions. If we used a model just based on the sentiment score of a review we'd get ok results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc515b0-a46c-4817-90ce-a3c21343bf5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "suppressMessages({\n",
    "    library(tidymodels)\n",
    "})\n",
    "\n",
    "tb_sentiment_predictions = tb_imdb_sentiment %>% \n",
    "    group_by(review_id) %>% \n",
    "    summarize(mu_sentiment = mean(sentiment), label = first(label)) %>% \n",
    "    mutate(prediction = ifelse(mu_sentiment < 0.5, \"negative\", \"positive\")) %>% \n",
    "    mutate(prediction = factor(prediction, levels = c(\"negative\", \"positive\")))\n",
    "\n",
    "tb_sentiment_predictions %>% \n",
    "    conf_mat(truth = label, estimate = prediction) \n",
    "\n",
    "tb_sentiment_predictions %>% \n",
    "    metrics(truth = label, estimate = prediction) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81e747a-891c-4e86-a4a4-01faeaecca9f",
   "metadata": {},
   "source": [
    "So if we predict the label (negative or positive) of a review based on the average sentiment score (< 0.5 is negative), then our model would have $\\approx 73\\%$ accuracy. Keep in mind that random guessing would yield $50\\%$ accuracy since the dataset is balanced. The [kap](https://en.wikipedia.org/wiki/Cohen%27s_kappa) metric accounts for random guessing with a value > 0 indicating better performance than random guessing.\n",
    "\n",
    "Now, let's produce some word clouds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08725681-4f49-424f-81bf-7fed68b0669a",
   "metadata": {},
   "source": [
    "## Word Clouds\n",
    "\n",
    "First, let's make a basic word cloud using the words that have a sentiment score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1a1295-464b-432e-b1a2-5cb3d091d27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "suppressMessages({\n",
    "    library(wordcloud)\n",
    "})\n",
    "\n",
    "tb_imdb_sentiment %>% \n",
    "    count(word, sort = T) %>% \n",
    "    slice(1:100) %>% # Get 250 words with the highest frequencies\n",
    "    with(wordcloud(word, n, color = \"gray50\")) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a514b5e0-3da4-4f1c-82a7-688e86847102",
   "metadata": {},
   "source": [
    "We can make this more informative by separating the words by positive or negative sentiment using the `comparison.cloud()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f15a05-dfea-4207-8e0e-cc9422fe0719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparison.cloud() expects a term matrix: rownames are words and columns are negative or positive\n",
    "getTermDocMatrix = function(tb_, num_words = 100) {\n",
    "    tb_tmp = tb_ %>% \n",
    "        mutate(sentiment = ifelse(sentiment == 0, \"negative\", \"positive\")) %>% \n",
    "        count(word, sentiment, sort = T) %>% \n",
    "        slice(1:num_words) %>% \n",
    "        pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) \n",
    "\n",
    "    M = tb_tmp %>% \n",
    "        select(negative, positive) %>% \n",
    "        as.matrix()\n",
    "\n",
    "    rownames(M) = tb_tmp$word\n",
    "\n",
    "    return(M)\n",
    "}\n",
    "\n",
    "tb_imdb_sentiment %>% \n",
    "    getTermDocMatrix() %>% \n",
    "    comparison.cloud(colors = c(\"#F8766D\", \"#00BFC4\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc14ca3-7748-4ed8-b115-c57c20309a24",
   "metadata": {},
   "source": [
    "## Classification Model\n",
    "\n",
    "Finally, let's train a classification model to identify positive and negative reviews. We'll use a logistic regression model here (hardly cutting edge, but still effective).\n",
    "\n",
    "> There's a lot going on in the next few lines of code. I encourage you to check out [this exerpt](https://smltar.com/mlclassification) from [Supervised Machine Learning for Text Analysis in R](https://smltar.com/) to dig deeper into each step of the process. This textbook is a wonderful resource, and this example in particular gives a great end-to-end example on using these tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503a2e65-cc05-4af6-9c06-5256ff268e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "suppressMessages({\n",
    "    library(textrecipes)\n",
    "})\n",
    "\n",
    "set.seed(1)\n",
    "\n",
    "# Reconstruct the reviews using only non-neutral language based on sentiment score\n",
    "tb_imdb_clean = tb_imdb_sentiment %>% \n",
    "    group_by(review_id) %>% \n",
    "    summarize(review_clean = str_c(word, collapse = \" \"), label = first(label))\n",
    "\n",
    "# get training and testing data\n",
    "imdb_split = initial_split(tb_imdb_clean, prop = 0.8)\n",
    "imdb_train = training(imdb_split)\n",
    "imdb_test  = testing(imdb_split)\n",
    "\n",
    "# 10-fold CV for hyperparameter tuning\n",
    "imdb_folds = vfold_cv(imdb_train)\n",
    "\n",
    "# specify the preprocessing\n",
    "imdb_rec = recipe(label ~ review_clean, data = tb_imdb_clean) %>% \n",
    "    step_tokenize(review_clean) %>%\n",
    "    step_tokenfilter(review_clean, max_tokens = 2E3) %>% # only use top 2K words\n",
    "    step_tfidf(review_clean)\n",
    "\n",
    "# sepecify the model\n",
    "lasso_spec = logistic_reg(penalty = tune(), mixture = 1) %>%\n",
    "  set_mode(\"classification\") %>%\n",
    "  set_engine(\"glmnet\")\n",
    "\n",
    "# define the hyperparameter grid\n",
    "lambda_grid = grid_regular(penalty(), levels = 10)\n",
    "\n",
    "# define the workflow\n",
    "tune_wf = workflow() %>%\n",
    "  add_recipe(imdb_rec) %>%\n",
    "  add_model(lasso_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b42cbf-10da-4c8c-9aa9-825d20fd99f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train the model & tune parameters\n",
    "# takes a few minutes to run...\n",
    "set.seed(1)\n",
    "tune_rs = tune_grid(\n",
    "  tune_wf,\n",
    "  imdb_folds,\n",
    "  grid = lambda_grid,\n",
    "  control = control_resamples(save_pred = TRUE, verbose = T)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9de275-ddb5-403f-bcb6-fbbe2a7c89d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "collect_metrics(tune_rs) # see all the accuracy metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ac1b11-3e40-48bf-af5d-26fc71fd0618",
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_rs %>% \n",
    "    show_best(metric = \"accuracy\") # see the most accuate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60825167-84c1-4318-af03-c5955ffe26a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the tuning parameter and train the model\n",
    "chosen_acc = tune_rs %>%\n",
    "  select_by_one_std_err(metric = \"accuracy\", -penalty)\n",
    "\n",
    "final_lasso = finalize_workflow(tune_wf, chosen_acc)\n",
    "\n",
    "fitted_lasso = fit(final_lasso, imdb_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ec8495-236f-4f89-ba4d-a844f53a0018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions from the test data\n",
    "tb_result = imdb_test %>% \n",
    "    mutate(prediction = predict(fitted_lasso, .) %>% unlist())\n",
    "\n",
    "tb_result %>% \n",
    "    conf_mat(truth = label, estimate = prediction) \n",
    "\n",
    "tb_result %>% \n",
    "    metrics(truth = label, estimate = prediction) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e341f10-114e-4bc2-b4f7-3aed1d213c4e",
   "metadata": {},
   "source": [
    "So we can achieve $\\approx 84\\%$ accuracy with our classification model. Not bad for a logistic regression model!\n",
    "\n",
    "## Going Further\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cf9f40-b723-4a5f-87d4-25f518851d0b",
   "metadata": {},
   "source": [
    "We could build a more accurate classification model using a deep learning framework. The [Supervised Machine Learning for Text Analysis in R](https://smltar.com/) textbook has [some chapters](https://smltar.com/dloverview) on building deep neural networks using keras. You can easily adapt one of these examples to build a model of your own using our IMDB dataset, though I'd reccomend installing the newer [keras3](https://keras3.posit.co/) if you try this out. As long as you follow the right steps, installation shouldn't be too much of a pain. I'll include a notebook in the \"Extra Materials\" section on Canvas to get you started with keras3 if you're interested. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
