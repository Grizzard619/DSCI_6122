{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ca39813-806e-43ee-9984-d77b600835b2",
   "metadata": {},
   "source": [
    "# Working with Text Data\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Text data is everywhere: social media posts, customer reviews, news articles, survey responses, medical records, legal documents, and more.  The ability to extract meaningful insights from this unstructured data is a crucial skill in data science.  Working with text enables us to perform tasks like:\n",
    "\n",
    "- **Sentiment Analysis:** Determining the emotional tone (positive, negative, neutral) of text. Used to understand customer opinions, brand perception, and public sentiment.\n",
    "- **Topic Modeling:** Discovering the underlying themes or topics discussed in a collection of documents. Useful for organizing large text corpora, identifying trends, and understanding customer feedback.\n",
    "- **Document Classification:** Categorizing documents into predefined classes (e.g., spam/not spam, news article categories).\n",
    "- **Named Entity Recognition:** Identifying and classifying named entities (people, organizations, locations, dates, etc.) in text.\n",
    "- **Building Chatbots and Virtual Assistants:** Natural language processing (NLP) is fundamental to these applications.\n",
    "- **Search Engines:** Building search engines and improving search relevance.\n",
    "\n",
    "This module will provide you with the foundational skills to effectively work with text data in R using `tidyverse` methods. While some base R functions can perform similar tasks, we will focus on the tidyverse approach for consistency, readability, and ease of use.\n",
    "\n",
    "### Learning Objectives\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "- Understand how text is represented in R.\n",
    "- Use the `stringr` package to perform common string manipulation tasks.\n",
    "- Apply basic regular expressions for pattern matching.\n",
    "- Convert text data into a tidy format using tidytext.\n",
    "- Perform fundamental text analysis using tidyverse tools.\n",
    "\n",
    "### Packages\n",
    "\n",
    "We'll make use of several different packages in this notebook:\n",
    "\n",
    "- `stringr`: Part of the tidyverse, stringr provides a consistent and user-friendly set of functions for working with strings. It simplifies many common text manipulation tasks. We will use this extensively.\n",
    "- `tidytext`: This package provides tools for converting text data into a tidy format (one-token-per-row), making it easy to integrate text analysis with other tidyverse workflows.\n",
    "\n",
    "You'll need to download the `tidytext` package before you go through this analysis. You should be familiar with how to download these through anaconda now, but I'll add a page on Canvas explaining the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a258dc5a-bfb6-4e2a-9732-9c22c20b09a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "suppressMessages({ # hide all the startup messages\n",
    "    library(tidyverse) # attaches stringr too\n",
    "    library(tidytext)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46793f43-dcca-4a2b-9265-765b36855a48",
   "metadata": {},
   "source": [
    "## Representing Text in R\n",
    "\n",
    "As you've seen in earlier modules, the fundamental way to represent text in R is through **character vectors**. Each element of the vector is a separate string. Even a single piece of text is typically represented as a character vector of length one. We can use the `str_length()` function to count the number of character in a string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2080ede-1b9b-4e27-b1c7-13af3799366b",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_string = \"This is a single string.\"\n",
    "\n",
    "cat(\"single string: \\n\")\n",
    "\n",
    "print(single_string)\n",
    "\n",
    "cat(\n",
    "    paste0(\n",
    "        \"\\nclass: \", class(single_string),\n",
    "        \"\\nlength: \", length(single_string),\n",
    "        \"\\nnum chars: \", str_length(single_string),\n",
    "        \"\\n\\n\"\n",
    "    )\n",
    ")\n",
    "\n",
    "cat(\"multiple strings: \\n\")\n",
    "\n",
    "multiple_strings = c(\"This is the first string.\", \"This is the second.\", \"And this is the third.\")\n",
    "print(multiple_strings)\n",
    "\n",
    "cat(\n",
    "    paste0(\n",
    "        \"\\nclass: \", class(multiple_strings),\n",
    "        \"\\nlength: \", length(multiple_strings),\n",
    "        \"\\nnum chars: \", paste0(str_length(multiple_strings), collapse = \",\")\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885b752a-8ff9-4806-b4c3-ff11394aa529",
   "metadata": {},
   "source": [
    "> In base R, there is also the function `nchar()` that can count the number of characters in a string. However, nchar() doesn't handle NA's or factors well, so you'd need to do a little extra work to ensure you're getting the right answer.\n",
    "\n",
    "You can use both single quotes (`'...'`) or double quotes (`\"...\"`) to create a string. The main difference arises when you want to include quotes within your string.  If you want a single quote inside your string, you can enclose the string in double quotes, and vice versa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770a9bc6-6826-4225-962b-9f6139d24298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single quote inside double quotes\n",
    "quote_example1 = \"It's a beautiful day.\\n\"\n",
    "cat(quote_example1)\n",
    "\n",
    "# Double quote inside single quotes\n",
    "quote_example2 = 'He said, \"Hello, world!\"'\n",
    "cat(quote_example2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36dea894-90e1-4606-b6d6-08b741f97943",
   "metadata": {},
   "source": [
    "What if you want both single and double quotes inside your string, or you need to include other special characters like newlines or tabs?  This is where escaping comes in. You've seen this already with `\\n` to create a newline; You use a backslash (\\) to \"escape\" the special meaning of the character that follows it. Here are some common escape seqquences:\n",
    "\n",
    "- `\\n`: Newline (starts a new line)\n",
    "- `\\t`: Tab (inserts a tab space)\n",
    "- `\\\\`: A literal backslash\n",
    "- `\\\"`: A literal double quote (when using double quotes to define the string)\n",
    "- `\\'`: A literal single quote (when using single quotes to define the string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb6d424-8e9c-4303-a58b-e3f532d4c7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "escaped_string = \"This is the first line.\\nThis is the second line.\\tThis is tabbed.\\nShe said, \\\"It's a beautiful day!\\\"\"\n",
    "cat(escaped_string) \n",
    "\n",
    "cat(\"\\n\\n\")\n",
    "\n",
    "escaped_string_single_quotes = 'What\\'s this, a literal backslash: \\\\ and did I just escape a single quote?!'\n",
    "cat(escaped_string_single_quotes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b586be-1209-46c6-8bac-9d2003c6b771",
   "metadata": {},
   "source": [
    "## Basic String Manipulation with `stringr`\n",
    "\n",
    "The `stringr` package, part of the `tidyverse`, provides a set of functions designed to make working with strings in R easier and more consistent.  `stringr` functions are vectorized, meaning they operate on entire character vectors at once, which is very efficient. They also generally follow a consistent naming convention: all `stringr` functions start with `str_`. We'll focus on common string manipulation tasks and the corresponding stringr functions.\n",
    "\n",
    "### Concatenation\n",
    "Concatenation is the process of joining strings together. We've seen how to do this using the `paste()` and `paste0()` functions. The primary function for this is in `stringr` is `str_c()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f776fca-a406-4467-8a7e-e85db20d0dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic concatenation\n",
    "string1 = \"Hello\"\n",
    "string2 = \"world\"\n",
    "combined_string = str_c(string1, string2)\n",
    "print(combined_string)\n",
    "\n",
    "# Using the 'sep' argument to add a separator\n",
    "combined_string_with_space = str_c(string1, string2, sep = \" \")\n",
    "print(combined_string_with_space)\n",
    "\n",
    "# Concatenating multiple strings\n",
    "apology = \"I'm sorry,\"\n",
    "name = \"Dave.\"\n",
    "explanation = \"I'm afraid I can't do that.\"\n",
    "hal_apology = str_c(apology, name, explanation, sep = \" \")\n",
    "print(hal_apology)\n",
    "\n",
    "# Using the 'collapse' argument to combine a vector into a single string\n",
    "words = c(\"What\", \"a\", \"strange\", \"movie.\")\n",
    "sentence = str_c(words, collapse = \" \")\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1391d9-6439-4215-93c3-04a7030f3bb0",
   "metadata": {},
   "source": [
    "`str_c()` is vectorized making it easy to handle certain operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7a69c8-857b-4b46-b80f-e11c0ec91ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorized concatenation\n",
    "first_names = c(\"Alice\", \"Bob\", \"Charlie\")\n",
    "last_names = c(\"Smith\", \"Jones\", \"Brown\")\n",
    "full_names = str_c(first_names, last_names, sep = \" \")\n",
    "print(full_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf414b79-8178-49ea-8510-d19a13483ef6",
   "metadata": {},
   "source": [
    "### Subsetting\n",
    "\n",
    "You can extract parts of strings using `str_sub()`. You specify the start and end positions (inclusive). Negative indices count from the end of the string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247d0e1f-0e0a-4491-bec8-830c52035d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting a substring\n",
    "text = \"This is an example string.\"\n",
    "substring1 = str_sub(text, 1, 4)  # Extract characters 1 to 4\n",
    "print(substring1)\n",
    "\n",
    "substring2 = str_sub(text, 6, 7)  # Extract characters 6 to 7\n",
    "print(substring2)\n",
    "\n",
    "substring3 = str_sub(text, -7, -1) # Extract the last 7 characters\n",
    "print(substring3)\n",
    "\n",
    "#str_sub works on vectors too.\n",
    "str_sub(full_names, 1, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6243d684-223d-425b-b250-82b8ba7baec9",
   "metadata": {},
   "source": [
    "### Case Conversion\n",
    "\n",
    "Change the case of strings using `str_to_lower()`, `str_to_upper()`, and `str_to_title()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67e8680-25fc-4a86-a662-9a548fc21a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case conversion\n",
    "mixed_case = \"ThIs Is MiXeD cAsE?\"\n",
    "lowercase = str_to_lower(mixed_case)\n",
    "print(lowercase)\n",
    "\n",
    "uppercase = str_to_upper(mixed_case)\n",
    "print(uppercase)\n",
    "\n",
    "titlecase = str_to_title(mixed_case)\n",
    "print(titlecase)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4266c8f-7ca4-4213-8442-85d712804005",
   "metadata": {},
   "source": [
    "### Whitespace Handling\n",
    "\n",
    "Remove unnecessary whitespace with `str_trim()` and `str_squish()`. `str_trim()` removes leading and trailing whitespace. `str_squish()` also removes extra spaces within the string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4bdc95-3c53-4c7f-aeaa-d3a346e89afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whitespace trimming\n",
    "messy_string = \"   Too much   whitespace!  \"\n",
    "trimmed_string = str_trim(messy_string)\n",
    "cat(trimmed_string, \"\\n\") # Shows more clearly the whitespace is removed.\n",
    "\n",
    "squished_string = str_squish(messy_string)\n",
    "cat(squished_string, \"\\n\")\n",
    "\n",
    "# str_trim() can also trim from only one side\n",
    "left_trimmed = str_trim(messy_string, side = \"left\")\n",
    "cat(\"Original: [\", messy_string, \"]\\n\", sep = \"\") # Show boundaries clearly\n",
    "cat(\"Left Trimmed: [\", left_trimmed, \"]\\n\", sep = \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a3639d-817b-4762-98af-70f2401a739f",
   "metadata": {},
   "source": [
    "### String Replacement\n",
    "\n",
    "`str_replace()` and `str_replace_all()` will replace matched patterns in your string(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3dfa09-6187-4ccd-a6c1-d0f95a9061f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_string = \"I like apples, apples are good.\"\n",
    "\n",
    "# Replace the first instance\n",
    "you_like_oranges_or_apples = str_replace(original_string, \"apples\", \"oranges\")\n",
    "print(you_like_oranges_or_apples)\n",
    "\n",
    "# Replace all instances\n",
    "you_like_oranges = str_replace_all(original_string, \"apples\", \"oranges\")\n",
    "print(you_like_oranges)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2304a6-df80-4910-b2e7-ccac122abca9",
   "metadata": {},
   "source": [
    "### String Duplication\n",
    "`str_dup()` will duplicate, then concatenate, a string a set number of times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcbd058-de5b-4f55-bfbe-8187b2ab7949",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_string = \"repeat me!\"\n",
    "\n",
    "#Duplicate a string 3 times\n",
    "duplicated_string = str_dup(my_string, 3)\n",
    "print(duplicated_string)\n",
    "\n",
    "#You can also provide a vector\n",
    "times = c(1,2,3)\n",
    "str_dup(my_string, times)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d55ac8-03d7-4856-a9fe-9b97a7a7cc8a",
   "metadata": {},
   "source": [
    "### String Padding\n",
    "\n",
    "`str_pad()` can be used to add padding to strings, such as spaces or zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c31908-c077-4e3f-b562-d8831ba4b4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_numbers = c(1, 10, 100, 1000)\n",
    "\n",
    "#pad numbers with zeros\n",
    "padded_numbers = str_pad(my_numbers, width = 4, side = \"left\", pad = \"0\")\n",
    "print(padded_numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc115eb-90e8-4e42-9630-0a4b5b5239eb",
   "metadata": {},
   "source": [
    "### Splitting Strings\n",
    "\n",
    "`str_split()` splits strings based on a delimiter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1537bb1f-8bce-4d16-90da-f7f46df1aa2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_string = \"This-is-my-delimited-string\"\n",
    "\n",
    "c(\"Split on the dash character.\\n\")\n",
    "split_string = str_split(my_string, \"-\")\n",
    "print(split_string)\n",
    "\n",
    "cat(\"\\nstr_split() also returns a list, because there can be different numbers of splits.\\n\")\n",
    "split_multiple = str_split(multiple_strings, \" \")\n",
    "print(split_multiple)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf017132-a7e8-4ec5-94d9-282afb770153",
   "metadata": {},
   "source": [
    "### Detecting Patterns\n",
    "\n",
    "You can use `str_detect()` to *detect* patterns in a string. It'll return a logical vector (TRUE/FALSE) indicating whether the pattern is found in each string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e59078-424e-4c5f-8f1e-c49bd81aa9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detecting the presence of \"apple\"\n",
    "fruits = c(\"apple\", \"banana\", \"orange\", \"grapefruit\")\n",
    "has_apple = str_detect(fruits, \"apple\")\n",
    "print(has_apple)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a4e2d8-2010-4c9d-8959-90176d2cda02",
   "metadata": {},
   "source": [
    "`str_starts()` and `str_ends()` are convenient shortcuts for checking if a string starts or ends with a specific pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e16498-1c05-4cec-96e7-d645e66b8ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using str_starts()\n",
    "starts_with_b = str_starts(fruits, \"b\")\n",
    "print(starts_with_b)\n",
    "\n",
    "# Using str_ends()\n",
    "ends_with_e = str_ends(fruits, \"e\")\n",
    "print(ends_with_e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11e2dcb-e327-447e-9797-0fa6f5760a4a",
   "metadata": {},
   "source": [
    "### Intro to Regular Expressions (regex)\n",
    "\n",
    "Regular expressions (regex) are a powerful way to describe patterns in text. `stringr` functions use regular expressions by default. We'll introduce some basic concepts here, but you can dig into [this chapter](https://r4ds.had.co.nz/strings.html#matching-patterns-with-regular-expressions) from Hadley Wickham's *R for Data Science* to learn more.  \n",
    "\n",
    "- **Patterns and Matching:** A regular expression is a sequence of characters that defines a search pattern. The goal is to match this pattern within a string.\n",
    "- **Basic Metacharacters:** These are some of the characters that have special meanings in regular expressions:\n",
    "    - `.` (dot): Matches any single character (except newline).\n",
    "    - `^`: Matches the beginning of the string.\n",
    "    - `$`: Matches the end of the string.\n",
    "    - `*`: Matches 0 or more repetitions of the preceding character/group.\n",
    "    - `+`: Matches 1 or more repetitions.\n",
    "    - `?`: Matches 0 or 1 repetition.\n",
    "    - `[]`: Character set (e.g., `[aeiou]` matches any vowel).\n",
    "    - `[^ ]`: Negated character set (e.g., `[^aeiou]` matches any non-vowel).\n",
    "    - `|`: OR operator (e.g., `a|b` matches either \"a\" or \"b\").\n",
    "    - `()`: Grouping.\n",
    "    - `\\d`: Match any digit from 0 to 9. You can specify a quantity with `{}`. \n",
    "- `regex()` **Modifiers:** Using the `regex()` helper function allows us to apply modifiers, such as case insensitivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b11c0d1-b40d-4a40-9b0f-9bb934e2b699",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat(\"Examples using metacharacters\\n\")\n",
    "text = c(\"apple\", \"banana\", \"apricot\", \"avocado\", \"Pineapple\")\n",
    "print(text)\n",
    "str_c(c(\"\\n\", str_dup(\"-\", 75), \"\\n\")) %>% cat()\n",
    "\n",
    "cat(\"Match strings starting with 'a'\\n\")\n",
    "str_detect(text, \"^a\") \n",
    "\n",
    "cat(\"\\nMatch strings ending with 'a'\\n\")\n",
    "str_detect(text, \"a$\")\n",
    "\n",
    "cat(\"\\nMatch strings containing 'app' followed by any character\\n\")\n",
    "str_detect(text, \"app.\")\n",
    "\n",
    "cat(\"\\nMatch strings containing some vowels\\n\")\n",
    "str_detect(text, \"[eio]\")\n",
    "\n",
    "cat(\"\\nCase-insensitive search for 'p'\\n\")\n",
    "str_detect(text, regex(\"p\", ignore_case = TRUE))\n",
    "\n",
    "cat(\"\\nMatch strings containing 'an' repeated one or more times\\n\")\n",
    "str_c(c(\"bannnnnana: \", str_detect(\"bannnnnana\", \"an+\"), \"\\n\")) %>% cat()\n",
    "str_c(c(\"bana: \", str_detect(\"bana\", \"an+\"), \"\\n\")) %>% cat()\n",
    "str_c(c(\"ba: \", str_detect(\"ba\", \"an+\"), \"\\n\")) %>% cat()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3bbae2-1423-446c-ae88-78804675805b",
   "metadata": {},
   "source": [
    "To match a literal metacharacter (like a literal dot `.`), you need to escape it with a backslash. Since the backslash itself is special in R strings, you need to use two backslashes (`\\\\`) to represent a single literal backslash in a regular expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a586761-c66e-46fc-bbd9-7934712f63de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matching a literal dot\n",
    "str_detect(\"This is a sentence with a period.\", \"\\\\.\")  # Matches because the sentence ends with a dot\n",
    "\n",
    "str_detect(\"This is a sentence without periods\", \".\") # Incorrect, matches ANY character.\n",
    "\n",
    "str_detect(\"This is a sentence without a period\", \"\\\\.\") # Correctly implemented"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d7fe28-b0e8-49e3-a67b-0fa7e15e5007",
   "metadata": {},
   "source": [
    "### Extracting Matches\n",
    "\n",
    "`str_extract()` extracts the *first* match of a pattern while `str_extract_all()` extracts *all* matches (returns a list). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9860640-3289-48db-a180-36a5a444300e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the first match\n",
    "text = \"My phone number is 555-123-4567, and my other number is 555-987-6543.\"\n",
    "first_phone_number = str_extract(text, \"\\\\d{3}-\\\\d{3}-\\\\d{4}\") # Matches a phone number format\n",
    "print(first_phone_number)\n",
    "\n",
    "# Extracting all matches (returns a list)\n",
    "all_phone_numbers = str_extract_all(text, \"\\\\d{3}-\\\\d{3}-\\\\d{4}\")\n",
    "print(all_phone_numbers)\n",
    "\n",
    "# str_extract_all returns a list because each element can have differing numbers of matches.\n",
    "another_example = c(\"One match here.\", \"Two matches here and here.\", \"No matches.\")\n",
    "str_extract_all(another_example, \"here\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87c993c-66d1-44bd-a273-c4e497b33ebf",
   "metadata": {},
   "source": [
    "## Intro to `tidytext`\n",
    "\n",
    "The `tidytext` package provides tools for converting text data into a tidy format.  The core idea of tidy data (as applied to text) is:\n",
    "\n",
    "- **One token per row:** Each row represents a single token, which is usually a word, but could also be a sentence, an [n-gram](https://en.wikipedia.org/wiki/N-gram), or another unit of text.\n",
    "- **Document-term matrix:** This tidy format facilitates creating a document-term matrix, which is a fundamental structure for many text analysis techniques.\n",
    "\n",
    "This tidy structure allows us to seamlessly integrate text analysis with other tidyverse packages like `dplyr` for filtering, counting, and summarizing.\n",
    "\n",
    "We'll focus on the key function `unnest_tokens()`. This function takes a data frame (or tibble) and converts a text column into a tidy format. Let's start with a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45223fc1-11c0-4233-83f1-89808461a46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a small data frame with a text column\n",
    "text_data = tibble(\n",
    "  document = 1:3,\n",
    "  text = c(\"This is the first document.\",\n",
    "           \"The second document is here.\",\n",
    "           \"And the third document.\")\n",
    ")\n",
    "\n",
    "# Tokenize the 'text' column into words\n",
    "tidy_text = text_data %>%\n",
    "  unnest_tokens(output = word, input = text)\n",
    "\n",
    "# Print the tidy data\n",
    "print(tidy_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66018172-c8f7-46f2-9f46-a8f9ce80f526",
   "metadata": {},
   "source": [
    "A lot happened in a few lines of code:\n",
    "\n",
    "- In `text_data`, we create an example where we imagine having multiple documents stored in our tibble.\n",
    "- The `unnest_tokens()` function performs the tokenization:\n",
    "    - `output = word`:  This argument specifies the name of the new column that will contain the tokens (the individual words by default). We're calling it `word`.\n",
    "    - `input = text`: This argument specifies the name of the column in our input tibble (`text_data`) that contains the text we want to tokenize. We're using the `text` column.\n",
    "\n",
    "There are a few other arguments we could specify in the `unnest_tokens()` function. The `token` argument is important as it controls how the text is split. The defaults is `\"words\"`, but some other options include:\n",
    "\n",
    "- \"sentences\": Splits the text into sentences.\n",
    "- \"characters\": Splits the text into individual characters.\n",
    "- \"ngrams\": Splits the text into n-grams (sequences of n words).\n",
    "- \"lines\": Splits the text into lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d97ed1-07eb-4c71-9188-7a4cf778693b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing by sentences\n",
    "sentences = text_data %>%\n",
    "  unnest_tokens(output = sentence, input = text, token = \"sentences\")\n",
    "print(sentences)\n",
    "\n",
    "# Tokenizing by characters\n",
    "characters = text_data %>%\n",
    "  unnest_tokens(output = char, input = text, token = \"characters\")\n",
    "print(characters)\n",
    "\n",
    "# Tokenizing into n-grams (in this case, bigrams - pairs of words)\n",
    "bigrams = text_data %>%\n",
    "  unnest_tokens(output = bigram, input = text, token = \"ngrams\", n = 2)\n",
    "print(bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e08ee2e-8f43-49c3-9d7f-b289c83b2933",
   "metadata": {},
   "source": [
    "Let's look at a simple but powerful example: counting word frequencies. We'll use a dataset from the [`janeaustenr`](https://cran.r-project.org/web/packages/janeaustenr/index.html) package that gives the text of Jane Austenâ€™s 6 completed, published novels as a one-row-per-line format. I've included the dataset in the `austen-books.rds` file, so make sure you have that downloaded before proceeding. An excellent analysis of this dataset is done in the [tidytext vignette](https://cran.r-project.org/web/packages/tidytext/vignettes/tidytext.html). I encourage you to check it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa6e38f-29e6-4b3a-b945-de14875b7457",
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_austen = readRDS(\"austen-books.rds\")\n",
    "\n",
    "tb_austen %>% \n",
    "    head()\n",
    "\n",
    "# Filter out common \"stop words\" (like \"the\", \"is\", \"a\")\n",
    "# using the built-in stop_words dataset from tidytext\n",
    "stop_words %>% \n",
    "    head()\n",
    "\n",
    "filtered_word_counts = tb_austen %>%\n",
    "    unnest_tokens(output = word, input = text) %>% \n",
    "    anti_join(stop_words, by = \"word\") %>%  # Remove stop words\n",
    "    count(word, sort = TRUE)\n",
    "\n",
    "filtered_word_counts %>% \n",
    "    head()\n",
    "\n",
    "options(repr.plot.width = 12, repr.plot.height = 8)\n",
    "theme_set(theme_bw(base_size = 16))\n",
    "\n",
    "# Visualize the top 10 words\n",
    "top_10_words = filtered_word_counts %>%\n",
    "  top_n(10) %>%\n",
    "  mutate(word = reorder(word, n)) %>% #Sort for plotting\n",
    "  ggplot(aes(x = word, y = n)) +\n",
    "  geom_col() +\n",
    "  coord_flip() +  # Horizontal bars\n",
    "  labs(title = \"Top 10 Words (Excluding Stop Words)\",\n",
    "       x = \"Word\",\n",
    "       y = \"Frequency\")\n",
    "print(top_10_words) #print for notebook, but really just need the statement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a531f3-5879-4415-8548-3bd50df6721d",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade0c557-c981-4cbc-a84a-7542ab527d4d",
   "metadata": {},
   "source": [
    "In this notebook, we've covered the fundamentals of working with text data in R, with a strong emphasis on the tidyverse approach. We learned:\n",
    "\n",
    "- **Text Representation**: How text is represented in R using character vectors.\n",
    "- **`stringr` Fundamentals**: How to use the `stringr` package for common string manipulation tasks like concatenation, subsetting, case conversion, whitespace handling, pattern detection (using basic regular expressions), and string extraction.\n",
    "- **Tidy Text with `tidytext`**: How to convert text data into a tidy format (one-token-per-row) using unnest_tokens(), enabling seamless integration with other tidyverse tools. We explored different tokenization options (\"words\", \"sentences\", \"characters\", \"ngrams\") and saw how to create a simple custom tokenizer.\n",
    "\n",
    "In the data wrangling notebook, we'll take a deeper dive on using some of the tools in `tidytext` and some other packages to do some more advanced analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
